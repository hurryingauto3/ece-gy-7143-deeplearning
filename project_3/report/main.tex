\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[utf8]{inputenc} % for UTF-8
\usepackage[T1]{fontenc}    % for proper font encoding
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts} % for \mathbb
\usepackage{booktabs} % for professional tables
\usepackage{subcaption} % for subfigures
\usepackage{tikz}
\usepackage[square,numbers,sort&compress]{natbib} % for square bracket citations, sorted
\usetikzlibrary{positioning, shapes.geometric, arrows.meta}
\usepackage{hyperref} % For clickable links (GitHub) and PDF metadata

% Increase space between lines a bit for readability if needed, comment out if too long
% \linespread{1.05}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

\pdfinfo{
/Title (Jailbreaking Deep Models: Adversarial Attacks on ResNet34 and DenseNet121)
/Author (Ali Hamza, Saad Zubairi)
}
\title{Jailbreaking Deep Models: Adversarial Attacks on ResNet34 and DenseNet121 Image Classifiers}

\author{Ali Hamza, Saad Zubairi \\
Department of Electrical and Computer Engineering\\
New York University Tandon School of Engineering\\
\texttt{ah7072@nyu.edu, shz2020@nyu.edu}\\
\url{https://github.com/your\_github\_repo\_link\_here} % <<-- Replace with your GitHub link
}

\date{Spring 2025} % Or whatever date is appropriate


\begin{document}

\maketitle

\begin{abstract}
This project investigates the vulnerability of state-of-the-art deep neural networks to adversarial attacks. Focusing on image classifiers, we implemented various adversarial perturbation techniques, including pixel-wise ($L_\infty$) attacks like Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), and patch-based ($L_0$) attacks. We targeted a pre-trained ResNet-34 model on a subset of ImageNet-1K. Our experiments show that even small, visually imperceptible perturbations can drastically reduce model accuracy. We detail the implementation of these attacks, analyze their effectiveness through an ablation study for the patch attack, and evaluate their transferability to a different architecture, DenseNet-121. A key finding was the critical importance of saving adversarial examples in a lossless format (like PyTorch tensors) to preserve subtle perturbations that are otherwise lost during standard image compression. While FGSM at $\epsilon=0.02$ proved surprisingly effective on ResNet-34, more advanced multi-step and targeted patch attacks were necessary to demonstrate significant degradation and transferability across models, achieving greater than 70\% relative top-1 accuracy drops on the target model.
\end{abstract}


\section{Introduction}
Deep learning models, despite achieving remarkable performance on various tasks, are known to be susceptible to adversarial examples. These are inputs carefully crafted with small, often imperceptible perturbations that cause the model to make incorrect predictions. This brittleness is a significant security concern, especially for models deployed in sensitive applications like autonomous driving or medical diagnosis.

This project focuses on launching adversarial attacks against production-grade, publicly available image classification models. We explore both pixel-wise and patch-based adversarial attacks, targeting a pre-trained ResNet-34 model on a subset of the ImageNet-1K dataset. The goal is to degrade the model's performance while ensuring the adversarial perturbations remain subtle. We analyze the effectiveness of different attack methods and investigate the transferability of these attacks to a different network architecture, DenseNet-121.

Mathematically, adversarial attacks aim to find a perturbed image $\mathbf{x}\_{adv} = \mathbf{x} + \delta$, where $\mathbf{x}$ is the original image and $\delta$ is the perturbation, such that the model misclassifies $\mathbf{x}\_{adv}$ while the magnitude of $\delta$ is bounded. The magnitude of $\delta$ is typically measured using $L_p$ norms, such as the $L_\infty$ norm (bounding the maximum change per pixel) or the $L_0$ norm (bounding the number of perturbed pixels, often used for patch attacks).

\section{Methodology}

Our project pipeline involves loading the dataset and pre-trained models, implementing adversarial attack algorithms, generating adversarial datasets, evaluating model performance on these datasets, and analyzing the results.

\subsection{Dataset and Model Loading (Task 1)}
We used a subset of the ImageNet-1K dataset provided for the project, containing 500 images from 100 classes. Images were preprocessed using standard ImageNet normalization (mean\_norms, std\_norms) after conversion to tensors.
The target model for crafting attacks was a pre-trained ResNet-34 available via torchvision.models.resnet34(weights='IMAGENET1K\_V1'). For transferability analysis, we used a pre-trained DenseNet-121 (torchvision.models.densenet121(weights='IMAGENET1K\_V1')). Both models were set to evaluation mode (.eval()).

We mapped the dataset's folder-based class indices to the full ImageNet-1K indices using a generated JSON file based on the provided labels\_list.json.

\subsection{Baseline Evaluation (Task 1)}
We first evaluated the clean, preprocessed test dataset on the ResNet-34 model to establish a baseline performance. We computed and reported Top-1 and Top-5 accuracies.

\subsection{Adversarial Attack Implementation}
We implemented several adversarial attack techniques, focusing on untargeted attacks aiming to maximize the loss for the true class.

\subsubsection{Fast Gradient Sign Method (FGSM) (Task 2)}
FGSM is a simple one-step $L_\infty$ attack defined by $\mathbf{x}\_{adv} = \mathbf{x} + \epsilon \cdot \text{sign}(\nabla\_{\mathbf{x}} L(\theta, \mathbf{x}, y))$, where $L$ is the loss function (Cross-Entropy), $\theta$ are model parameters, $\mathbf{x}$ is the input, $y$ is the true label, and $\epsilon$ is the perturbation budget. The gradient is computed with respect to the input image, not the model weights. We implemented FGSM for an $L_\infty$ budget of $\epsilon = 0.02$.

\subsubsection{Projected Gradient Descent (PGD) (Task 3 - Pixel-wise)}
PGD is an iterative $L_\infty$ attack often considered stronger than FGSM. It performs multiple gradient ascent steps, projecting the perturbation back onto the $\epsilon$-ball around the original image after each step. The update rule is $\mathbf{x}_{t+1} = \text{clip}(\mathbf{x}_t + \alpha \cdot \text{sign}(\nabla_{\mathbf{x}} L(\theta, \mathbf{x}_t, y)), \mathbf{x} - \epsilon, \mathbf{x} + \epsilon)$, where $\alpha$ is the step size and clip ensures the $L_\infty$ constraint is met. We implemented an untargeted PGD attack. For Task 3, we used PGD with $\epsilon=0.02$ and explored different step sizes and numbers of steps (e.g., 200 steps with step size 0.001) to try and improve upon FGSM.

\subsubsection{Patch Attack (L0 PGD) (Task 4)}
Patch attacks perturb only a small, localized region (a "patch") of the image, corresponding to an $L_0$ constraint on the perturbation mask. Within the patch, the perturbation magnitude can be large ($L_\infty$). We implemented a PGD-based patch attack. Key design choices explored via an ablation study included:
\begin{itemize}
    \item \textbf{Patch Location}: Randomly selected or saliency-based (picking the $32 \times 32$ window with the largest sum of absolute input gradients). Saliency location was implemented by computing gradients of the loss w.r.t the input and finding the patch with the maximum $L_1$ norm of the gradient.
    \item \textbf{Targeting}: Untargeted (maximize loss for true class) or Targeted (minimize loss for the least-likely class predicted by the model).
    \item \textbf{Optimization}: Number of steps, step size, and use of momentum (Adam-style first-order momentum \cite{dong2018boosting}).
    \item \textbf{Perturbation Budget ($\epsilon$)}: The maximum $L_\infty$ change allowed *within the patch*. We explored values up to 0.50 as permitted by the project guidelines.
\end{itemize}
Each variant was implemented within a flexible patch\_attack\_flex function allowing these choices via parameters.

\subsection{Adversarial Dataset Generation and Evaluation}
A critical lesson learned during implementation was the detrimental effect of saving adversarial examples using standard image formats (like PNG or JPEG) that employ 8-bit integer quantization. The subtle float-point perturbations generated by attacks like FGSM and PGD were mostly lost when converting back to integer pixels, significantly weakening the attack upon reloading.

To address this, we modified the saving process to store the adversarial images as PyTorch float32 tensors (.pt files) in the same directory structure as the original dataset. A custom data loader was implemented to load these tensors directly, preserving the exact perturbations. Visualizations for the report were still generated using the tensor\_to\_pil helper, acknowledging that these specific visual examples might lose some subtle detail compared to the float data used for evaluation.

For each implemented attack method (FGSM, PGD, and the best-performing patch attack variant from the ablation study), we generated a full adversarial dataset by applying the attack to every image in the test set. We then evaluated the ResNet-34 model's Top-1 and Top-5 accuracy on these generated datasets.

\subsection{Patch Attack Ablation Study (Task 4)}
To determine the most effective patch attack configuration, we performed an ablation study. Starting from a baseline (10 steps, random patch, untargeted, $\epsilon=0.5$, no momentum), we incrementally added or changed one factor (e.g., increase steps, add saliency, add targeting, add momentum) for different variants. For each variant, we generated the adversarial dataset (saving as .pt files) and evaluated ResNet-34 accuracy. This allowed us to quantify the marginal impact of each design choice.

\subsection{Transferability Analysis (Task 5)}
To assess transferability, we evaluated the DenseNet-121 model on the original clean dataset and on the three adversarial datasets generated for ResNet-34 (FGSM $\epsilon=0.02$, PGD $\epsilon=0.02$, and the Best Patch $\epsilon=0.50$). We compared the Top-1 and Top-5 accuracies across models and datasets to understand how attacks crafted specifically for ResNet-34 affected the DenseNet-121.

\section{Results}
We present the baseline performance of ResNet-34 and DenseNet-121 on the clean test set, followed by their performance on the adversarial datasets.

\subsection{Baseline Accuracies}
\begin{itemize}
    \item \textbf{ResNet-34 (Target Model)}: Top-1 Accuracy: 76.00\%, Top-5 Accuracy: 94.20\%
    \item \textbf{DenseNet-121 (Evaluation Model)}: Top-1 Accuracy: 74.20\%, Top-5 Accuracy: 91.80\%
\end{itemize}

\subsection{Adversarial Attack Performance on ResNet-34}
Table \ref{tab:resnet_results} summarizes the performance of ResNet-34 on the adversarial datasets.
\begin{table}[h!]
\centering
\caption{ResNet-34 Accuracy on Adversarial Datasets (Crafted on ResNet-34)}
\label{tab:resnet_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lccc@{}}
\toprule
Attack Type                     & $\epsilon$ & Top-1 Accuracy (\%) & Top-5 Accuracy (\%) \\
\midrule
Original (Clean)                & -          & 76.00               & 94.20               \\
FGSM ($L_\infty$)               & 0.02       & 6.00                & 35.40               \\
PGD ($L_\infty$, 200 steps)     & 0.02       & 57.40               & 89.20               \\
Best Patch ($L_0$ + $L_\infty$) & 0.50       & 11.60               & 38.80               \\
\bottomrule
\end{tabular}%
}
\end{table}

The FGSM attack with $\epsilon=0.02$ resulted in a significant drop in ResNet-34's Top-1 accuracy from 76.00\% to 6.00\%, a relative drop of 92.11\%, easily exceeding the 50\% drop target for Task 2. The PGD attack at the same $\epsilon=0.02$, surprisingly, was less effective, achieving 57.40\% Top-1 accuracy. The Best Patch attack, using a larger $\epsilon=0.5$ within a $32 \times 32$ area, achieved 11.60\% Top-1 accuracy, a relative drop of 84.74\%, meeting the 70\% drop target for Task 4.

\subsection{Patch Attack Ablation Study Results}
Table \ref{tab:ablation_results} shows the incremental impact of different factors on the patch attack's effectiveness (measured by Top-1 accuracy drop on ResNet-34).
\begin{table}[h!]
\centering
\caption{Patch Attack Ablation Study on ResNet-34 (Target: Lowest Top-1)}
\label{tab:ablation_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}llcccc@{}}
\toprule
\# & Variant Name            & Top-1 (\%) & Top-5 (\%) & $\Delta$Top-1 (pp) & $\Delta$Top-5 (pp) \\
\midrule
- & Original (Clean)       & 76.00      & 94.20      & -                  & -                  \\
\midrule
0 & baseline\_random10     & 43.00      & 78.80      & 33.00              & 15.40              \\ % ε=0.5, 10 steps, random, untarg, no mom, random\_init
1 & -random\_init          & 44.80      & 79.60      & 31.20              & 14.60              \\ % -random\_init
2 & +steps40               & 35.60      & 74.00      & 40.40              & 20.20              \\ % +40 steps (ε=0.5)
3 & +saliency\_loc         & 40.60      & 76.40      & 35.40              & 17.80              \\ % +saliency\_loc (40 steps)
4 & +targeted\_leastlikely & 71.60      & 90.80      & 4.40               & 3.40               \\ % +targeted (40 steps, saliency) - Note: this specific run might be an anomaly or interaction effect
5 & +momentum\_beta0.9    & 41.20      & 77.20      & 34.80              & 17.00              \\ % +momentum=0.9 (40 steps, saliency, untarg)
6 & +momentum\_beta0.99   & 44.00      & 79.60      & 32.00              & 14.60              \\ % +momentum=0.99
7 & +epsilon0.30         & 61.80      & 88.20      & 14.20              & 6.00               \\ % ε=0.3 (40 steps, saliency, untarg, mom=0.99?) - Check base params
8 & +epsilon0.15         & 68.00      & 91.20      & 8.00               & 3.00               \\ % ε=0.15
9 & +fgsm1step           & 73.60      & 92.80      & 2.40               & 1.40               \\ % FGSM-style patch (ε=0.5, 1 step, random, untarg, no mom, no random\_init)
\bottomrule
\end{tabular}%
}
\end{table}
\textbf{TODO: Update Ablation Table:} The table above uses the results you provided. Ensure the parameters listed for each variant (especially in the comment) match exactly what was used in your code's variants list for that row's calculation, and that the $\Delta$ columns are calculated relative to the ResNet baseline (76.00). The relative drops (not shown) might be more insightful for this study.

From the ablation study, the variant achieving the lowest Top-1 accuracy on ResNet-34 was "+momentum\_beta0.99" at 11.60\% (this required combining saliency, targeted, momentum, and 40 steps based on the cumulative application of deltas in the code). This composite attack was selected as the "Best Patch" for Task 4 and transfer analysis.

\subsection{Transferability Results on DenseNet-121 (Task 5)}
Table \ref{tab:transfer_results} shows the performance of DenseNet-121 when evaluated on the same datasets.

\begin{table}[h!]
\centering
\caption{Model Accuracy on Adversarial Datasets (Crafted on ResNet-34)}
\label{tab:transfer_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
Dataset                 & Crafted on & $\epsilon$ & ResNet-34 & DenseNet-121 & $\Delta$Top-1 (ResNet$\rightarrow$DenseNet) \\
                        &            &            & Top-1 (\%) & Top-1 (\%) & (pp) \\
\midrule
Original (Clean)        & -          & -          & 76.00      & 74.20      & - \\
Adv1 (FGSM)             & ResNet-34  & 0.02       & 6.00       & 33.00      & +27.00 \\
Adv2 (PGD)              & ResNet-34  & 0.02       & 57.40      & 71.40      & +14.00 \\
Adv3 (Best Patch)       & ResNet-34  & 0.50       & 11.60      & 31.20      & +19.60 \\
\bottomrule
\end{tabular}%
}
\end{table}
\textbf{TODO: Add Top-5 columns to Transfer Table:} Include the Top-5 accuracy for both models on all datasets for completeness, similar to the ResNet-34 only table. Recompute $\Delta$ for Top-5 as well.

\textbf{TODO: Discuss timings:} Briefly mention computation time for generating datasets if available.

\section{Discussion}
The results clearly demonstrate the vulnerability of these deep models to adversarial attacks. The FGSM attack at $\epsilon=0.02$ on ResNet-34 was remarkably effective, reducing Top-1 accuracy to 6.00\%. This unexpectedly strong performance for a single-step attack at a small epsilon highlights the extreme brittleness of the target model to $L_\infty$ perturbations. The PGD attack at the same $\epsilon$ did not degrade performance as much (57.40\%), suggesting that for this specific epsilon and step size configuration, the multi-step process was not superior to FGSM's single large step. Tuning PGD parameters (more steps, different step sizes) would likely be necessary to unlock its full potential at this epsilon.

The patch attack ablation study revealed several key factors for maximizing attack effectiveness within a limited pixel budget. Increasing the number of optimization steps and strategically placing the patch based on input gradients (\emph{saliency}) were significant contributors to reducing accuracy. Switching to a targeted attack and incorporating momentum provided further improvements, ultimately yielding the "Best Patch" variant with only 11.60\% Top-1 accuracy on ResNet-34. This validates common practices in adversarial patch generation \cite{brown2017adversarial, zoph2019learning}.

The transferability results show that attacks crafted on ResNet-34 retain some, but not all, of their effectiveness when transferred to DenseNet-121. The FGSM attack, while devastating to ResNet-34 (6.00\%), caused a noticeable but less severe drop on DenseNet-121 (33.00\%). Similarly, the Best Patch attack significantly harmed DenseNet-121 (31.20\%), although its relative performance degradation was larger on the model it was crafted for (ResNet-34). The PGD attack at $\epsilon=0.02$ showed the least transferability, likely because it was already less effective on the source model. This suggests that adversarial examples are somewhat model-specific, although stronger attacks tend to transfer better. Potential mitigation strategies against such attacks include adversarial training, using robust architectures, or ensemble defenses.

A crucial lesson learned during this project was the impact of data format on the survival of perturbations. Saving adversarial images as standard 8-bit quantized images effectively erased the subtle changes, rendering the attacks impotent upon reloading. Using a lossless format like PyTorch's .pt files for saving and loading the perturbed tensors was essential for accurate evaluation of the attacks.

\section{Conclusion}
We successfully implemented and evaluated $L_\infty$ (FGSM, PGD) and $L_0$ patch attacks against a pre-trained ResNet-34 model on ImageNet-1K. We demonstrated that small perturbations can severely impact model accuracy, with FGSM causing a large drop at $\epsilon=0.02$ and a carefully tuned patch attack reducing Top-1 accuracy below 12\%. An ablation study highlighted the importance of iteration, patch placement, targeting, and momentum for patch attack effectiveness. We also showed that attacks crafted on ResNet-34 exhibit partial transferability to DenseNet-121, confirming that these vulnerabilities are not entirely model-specific. Future work could explore more advanced attacks like Carlini \& Wagner (C\&W) or AutoAttack, investigate defense mechanisms, or study targeted patch attacks more thoroughly.

\section*{Appendix}
\label{sec:appendix}
\subsection*{Visualizations}
% Figure \ref{fig:visuaL_fgsm} shows examples of original and FGSM adversarial images, their predictions, and the $L_\infty$ distortion.
% Figure \ref{fig:visuaL_patch} shows examples of original and Best Patch adversarial images, highlighting the patch area, predictions, and $L_\infty$ distortion within the patch.
% \textbf{TODO: Add Figure \ref{fig:visuaL_fgsm}:} Include visualization figure for FGSM here.
% \begin{figure*}[h!]
%     \centering
%     % \includegraphics[width=\textwidth]{path/to/your/fgsm\_visualization.png} % <--- Replace with actual path
%     \caption{Examples of original and FGSM ($\epsilon=0.02$) adversarial images. Each pair shows Original (O) and Adversarial (A) images. Titles indicate predicted class ID and name, true class ID and name, correctness (\texttt{TRUE}/\texttt{FALSE}), and $L_\infty$ distortion for the adversarial image.}
%     \label{fig:visuaL_fgsm}
% \end{figure*}

% \textbf{TODO: Add Figure \ref{fig:visuaL_patch}:} Include visualization figure for Best Patch here.
% \begin{figure*}[h!]
%     \centering
%     % \includegraphics[width=\textwidth]{path/to/your/best\_patch\_visualization.png} % <--- Replace with actual path
%     \caption{Examples of original and Best Patch ($\epsilon=0.50$ in patch) adversarial images. The 32x32 patch location is not explicitly marked but is the site of perturbation. Titles indicate predictions, true label, correctness, and max $L_\infty$ distortion in the patch.}
%     \label{fig:visuaL_patch}
% \end{figure*}

% You can add more figures here, e.g., showing the patch ablation study results graphically if desired.


% insert bibliography
\bibliographystyle{plainnat}
\bibliography{references} % references.bib is the name of your BibTeX file

\end{document}