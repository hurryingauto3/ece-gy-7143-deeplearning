{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKRRjmhbFcx7"
   },
   "source": [
    "In this problem we will train a neural network from scratch using numpy. In practice, you will never need to do this (you'd just use TensorFlow or PyTorch). But hopefully this will give us a sense of what's happening under the hood. \n",
    "\n",
    "For training/testing, we will use the standard MNIST benchmark consisting of images of handwritten images. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Zpyb4xQNu2"
   },
   "source": [
    "In the second demo, we worked with autodiff. Autodiff enables us to implicitly store how to calculate the gradient when we call backward. We implemented some basic operations (addition, multiplication, power, and ReLU). In this homework problem, you will implement backprop for more complicated operations directly. Instead of using autodiff, you will manually compute the gradient of the loss function for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kjvPSnDA4J_w"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "plt.imshow(x_train[0],cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Klx9qPmxF9jI"
   },
   "source": [
    "Loading MNIST is the only place where we will use TensorFlow; the rest of the code will be pure numpy.\n",
    "\n",
    "Let us now set up a few helper functions. We will use sigmoid activations for neurons, the softmax activation for the last layer, and the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "sdyvaUKoF7ux"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Numerically stable sigmoid function based on\n",
    "  # http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "  \n",
    "  x = np.clip(x, -500, 500) # We get an overflow warning without this\n",
    "  \n",
    "  return np.where(\n",
    "    x >= 0,\n",
    "    1 / (1 + np.exp(-x)),\n",
    "    np.exp(x) / (1 + np.exp(x))\n",
    "  )\n",
    "\n",
    "def dsigmoid(x): # Derivative of sigmoid\n",
    "  return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "  # Numerically stable softmax based on (same source as sigmoid)\n",
    "  # http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "  b = x.max()\n",
    "  y = np.exp(x - b)\n",
    "  return y / y.sum()\n",
    "\n",
    "def cross_entropy_loss(y, yHat):\n",
    "  return -np.sum(y * np.log(yHat))\n",
    "\n",
    "def integer_to_one_hot(x, max):\n",
    "  # x: integer to convert to one hot encoding\n",
    "  # max: the size of the one hot encoded array\n",
    "  result = np.zeros(10)\n",
    "  result[x] = 1\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIZEupTHyNM"
   },
   "source": [
    "OK, we are now ready to build and train our model. The input is an image of size 28x28, and the output is one of 10 classes. So, first: \n",
    "\n",
    "Q1. Initialize a 2-hidden layer neural network with 32 neurons in each hidden layer, i.e., your layer sizes should be: \n",
    "\n",
    "784 -> 32 -> 32 -> 10\n",
    "\n",
    "If the layer is $n_{in} \\times n_{out}$ your layer weights should be initialized by sampling from a normal distribution with mean zero and variance 1/$\\max(n_{in},n_{out})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight and Bias Initialization\n",
    "\n",
    "- The bias term for each layer is initialized to zero.\n",
    "- The weights for each layer are initialized by sampling from a normal distribution with mean zero and variance that depends on the size of the layer. We can use various strategies to define the standard deviation of the normal distribution. A common one is to use 1/$\\sqrt max(n_{in},n_{out})$, where $n_{in}$ and $n_{out}$ are the number of input and output units for the layer. Although, there are a few others that can also be used and may affect model performance. 1/$\\max{n_{in},n_{out}}$ had lower performance in my experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "zBeGvbu6FaM_"
   },
   "outputs": [],
   "source": [
    "# # Initialize weights of each layer with a normal distribution of mean 0 and\n",
    "# # standard deviation 1/sqrt(n), where n is the number of inputs.\n",
    "# # This means the weighted input will be a random variable itself with mean\n",
    "# # 0 and standard deviation close to 1 (if biases are initialized as 0, standard\n",
    "# # deviation will be exactly 1)\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(80085)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def standard_deviation(n_in, n_out, method='xavier'):\n",
    "    if method == 'sqrtmax':\n",
    "        return 1 / np.sqrt(max(n_in, n_out))\n",
    "    if method == 'max':\n",
    "        return 1 / max(n_in, n_out)\n",
    "    if method == 'xavier':\n",
    "        return 1 / np.sqrt(n_in)\n",
    "    if method == 'he':\n",
    "        return 2 / np.sqrt(n_in)\n",
    "    if method == 'he2':\n",
    "        return 2 / np.sqrt(n_in + n_out)\n",
    "    raise ValueError('Invalid method')\n",
    "\n",
    "# Network architecture: 784 -> 32 -> 32 -> 10\n",
    "input_size = 28 * 28  # 784\n",
    "hidden_size = 32\n",
    "output_size = 10\n",
    "\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "# Layer 1: Input (784) -> Hidden1 (32)\n",
    "n_in, n_out = input_size, hidden_size\n",
    "std = standard_deviation(n_in, n_out, 'sqrtmax')\n",
    "weights.append(np.random.randn(n_out, n_in) * std)  # shape: (32,784)\n",
    "biases.append(np.zeros(n_out))\n",
    "\n",
    "# Layer 2: Hidden1 (32) -> Hidden2 (32)\n",
    "n_in, n_out = hidden_size, hidden_size\n",
    "std = standard_deviation(n_in, n_out, 'sqrtmax')\n",
    "weights.append(np.random.randn(n_out, n_in) * std)  # shape: (32,32)\n",
    "biases.append(np.zeros(n_out))\n",
    "\n",
    "# Layer 3: Hidden2 (32) -> Output (10)\n",
    "n_in, n_out = hidden_size, output_size\n",
    "std = standard_deviation(n_in, n_out, 'sqrtmax')\n",
    "weights.append(np.random.randn(n_out, n_in) * std)  # shape: (10,32)\n",
    "biases.append(np.zeros(n_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IafUGD_VGeLh"
   },
   "source": [
    "Next, we will set up the forward pass. We will implement this by looping over the layers and successively computing the activations of each layer. \n",
    "\n",
    "Q2. Implement the forward pass for a single sample, and for the entire dataset.\n",
    "\n",
    "\n",
    "Right now, your network weights should be random, so doing a forward pass with the data should not give you any meaningful information. Therefore, in the last line, when you calculate test accuracy, it should be somewhere around 1/10 (i.e., a random guess)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass \n",
    "\n",
    "In this step, we will implement the forward pass of the neural network for a single sample and for the entire dataset. \n",
    "\n",
    "The forward pass for the single step is given by the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "z_1 &= xW_1 + b_1 \\\\\n",
    "a_1 &= \\sigma(z_1) \\\\\n",
    "z_2 &= a_1W_2 + b_2 \\\\\n",
    "a_2 &= \\sigma(z_2) \\\\\n",
    "z_3 &= a_2W_3 + b_3 \\\\\n",
    "a_3 &= \\text{softmax}(z_3)\n",
    "\\end{align}\n",
    "\n",
    "where $x$ is the input, $W_i$ and $b_i$ are the weights and biases for the $i$-th layer, $\\sigma$ is the sigmoid activation function, and $\\text{softmax}$ is the softmax activation function.\n",
    "\n",
    "The softmax function simply applies the exponential function to each element of the input vector and then normalizes the output vector to have a sum of 1\n",
    "\n",
    "This step simply calculates the inference of the neural network on the test and training data based on the weights and biases that we have initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "cd6jGroQGdOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 2.43\n",
      "Accuracy (# of correct guesses): 989.0 / 10000 ( 9.89 %)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def feed_forward_sample(sample, y):\n",
    "    \"\"\"\n",
    "    Forward pass for a single sample.\n",
    "    Inputs:\n",
    "      sample: a 2D numpy array (28x28) for the MNIST digit.\n",
    "      y: integer label (0-9).\n",
    "    Returns:\n",
    "      loss: cross entropy loss for this sample.\n",
    "      one_hot_guess: one-hot encoded prediction (vector of length 10).\n",
    "    \"\"\"\n",
    "    # Flatten the sample into a vector of size 784\n",
    "    x = sample.flatten()  # shape (784,)\n",
    "    \n",
    "    # Layer 1: hidden layer\n",
    "    z1 = np.dot(weights[0], x) + biases[0]      # shape (128,)\n",
    "    a1 = sigmoid(z1)                            # shape (128,)\n",
    "    \n",
    "    # Layer 2: output layer\n",
    "    z2 = np.dot(weights[1], a1) + biases[1]       # shape (10,)\n",
    "    a2 = sigmoid(z2)                            # shape (10,)\n",
    "\n",
    "     # Layer 3 forward\n",
    "    z3 = np.dot(weights[2], a2) + biases[2]\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    # Compute loss using cross-entropy; convert y to one-hot\n",
    "    y_one_hot = integer_to_one_hot(y, output_size)\n",
    "    loss = cross_entropy_loss(y_one_hot, a3)\n",
    "    \n",
    "    # Get prediction as one-hot vector\n",
    "    pred_label = np.argmax(a3)\n",
    "    one_hot_guess = np.zeros(output_size)\n",
    "    one_hot_guess[pred_label] = 1\n",
    "    \n",
    "    return loss, one_hot_guess\n",
    "\n",
    "\n",
    "def feed_forward_dataset(x, y):\n",
    "  losses = np.empty(x.shape[0])\n",
    "  one_hot_guesses = np.empty((x.shape[0], output_size))\n",
    "\n",
    "  for i in range(x.shape[0]):\n",
    "          loss, one_hot_guess = feed_forward_sample(x[i], y[i])\n",
    "          losses[i] = loss\n",
    "          one_hot_guesses[i] = one_hot_guess\n",
    "  \n",
    "  y_one_hot = np.zeros((y.size, 10))\n",
    "  y_one_hot[np.arange(y.size), y] = 1\n",
    "\n",
    "  correct_guesses = np.sum(y_one_hot * one_hot_guesses)\n",
    "  correct_guess_percent = format((correct_guesses / y.shape[0]) * 100, \".2f\")  \n",
    "\n",
    "  print(\"\\nAverage loss:\", np.round(np.average(losses), decimals=2))\n",
    "  print(\"Accuracy (# of correct guesses):\", correct_guesses, \"/\", y.shape[0], \"(\", correct_guess_percent, \"%)\")\n",
    "\n",
    "def feed_forward_training_data():\n",
    "  print(\"Feeding forward all training data...\")\n",
    "  feed_forward_dataset(x_train, y_train)\n",
    "  print(\"\")\n",
    "\n",
    "def feed_forward_test_data():\n",
    "  print(\"Feeding forward all test data...\")\n",
    "  feed_forward_dataset(x_test, y_test)\n",
    "  print(\"\")\n",
    "\n",
    "feed_forward_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSrlc2VLOi8L"
   },
   "source": [
    "OK, now we will implement the backward pass using backpropagation. We will keep it simple and just do training sample-by-sample (no minibatching, no randomness).\n",
    "\n",
    "Q3: Compute the gradient of all the weights and biases by backpropagating derivatives all the way from the output to the first layer. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: Forward and Backward Pass\n",
    "\n",
    "- The forward pass computes the predicted output for a given input.\n",
    "-- The forward pass implemented here is the same as the one we implemented in the previous step\n",
    "- The backward pass computes the gradient of the loss with respect to the weights and biases.\n",
    "-- The backward pass is implemented here by computing the gradients of the loss with respect to the weights and biases of each layer. The gradients are computed by backpropagating the derivatives from the output layer to  the input layer. The gradients are then used to update the weights and biases of the network using the gradient descent algorithm. The equations for the gradients are derived using the chain rule. The equation is as follows:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "BLLEsVdcOgzi"
   },
   "outputs": [],
   "source": [
    "def train_one_sample(sample, y, learning_rate=0.003):\n",
    "    x = sample.flatten()  # shape: (784,)\n",
    "    \n",
    "    # Forward pass\n",
    "    z1 = np.dot(weights[0], x) + biases[0]  # (32,)\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = np.dot(weights[1], a1) + biases[1]   # (32,)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.dot(weights[2], a2) + biases[2]   # (10,)\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    y_one_hot = integer_to_one_hot(y, output_size)\n",
    "    loss = cross_entropy_loss(y_one_hot, a3)\n",
    "    \n",
    "    # Backward pass:\n",
    "    # Output layer: derivative of softmax-crossentropy\n",
    "    delta3 = a3 - y_one_hot                  # (10,)\n",
    "    dW3 = np.outer(delta3, a2)               # (10,32)\n",
    "    db3 = delta3\n",
    "    \n",
    "    # Backpropagate to second hidden layer:\n",
    "    delta2 = np.dot(weights[2].T, delta3) * dsigmoid(z2)  # (32,)\n",
    "    dW2 = np.outer(delta2, a1)                            # (32,32)\n",
    "    db2 = delta2\n",
    "    \n",
    "    # Backpropagate to first hidden layer:\n",
    "    delta1 = np.dot(weights[1].T, delta2) * dsigmoid(z1)  # (32,)\n",
    "    dW1 = np.outer(delta1, x)                             # (32,784)\n",
    "    db1 = delta1\n",
    "    \n",
    "    # Update weights and biases\n",
    "    weights[2] -= learning_rate * dW3\n",
    "    biases[2] -= learning_rate * db3\n",
    "    weights[1] -= learning_rate * dW2\n",
    "    biases[1] -= learning_rate * db2\n",
    "    weights[0] -= learning_rate * dW1\n",
    "    biases[0] -= learning_rate * db1\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AXY27pOB9cW"
   },
   "source": [
    "Finally, train for 3 epochs by looping over the entire training dataset 3 times.\n",
    "\n",
    "Q4. Train your model for 3 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: Looping Over the Dataset\n",
    "\n",
    "The model is trained by looping over the entire training dataset for a fixed number of epochs. The forward and backward passes are computed for each sample in the dataset. The gradients are then used to update the weights and biases of the network using the gradient descent algorithm. The loss is computed for each sample and the average loss is computed for the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "8Ygk05FcB-rL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 2.43\n",
      "Accuracy (# of correct guesses): 989.0 / 10000 ( 9.89 %)\n",
      "\n",
      "Training for one epoch over the training dataset...\n",
      "Processed 10000/60000 samples\n",
      "Processed 20000/60000 samples\n",
      "Processed 30000/60000 samples\n",
      "Processed 40000/60000 samples\n",
      "Processed 50000/60000 samples\n",
      "Processed 60000/60000 samples\n",
      "Finished training epoch. Average Loss: 1.2322 \n",
      "\n",
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 1.05\n",
      "Accuracy (# of correct guesses): 6302.0 / 10000 ( 63.02 %)\n",
      "\n",
      "Training for one epoch over the training dataset...\n",
      "Processed 10000/60000 samples\n",
      "Processed 20000/60000 samples\n",
      "Processed 30000/60000 samples\n",
      "Processed 40000/60000 samples\n",
      "Processed 50000/60000 samples\n",
      "Processed 60000/60000 samples\n",
      "Finished training epoch. Average Loss: 0.9951 \n",
      "\n",
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 0.96\n",
      "Accuracy (# of correct guesses): 6773.0 / 10000 ( 67.73 %)\n",
      "\n",
      "Training for one epoch over the training dataset...\n",
      "Processed 10000/60000 samples\n",
      "Processed 20000/60000 samples\n",
      "Processed 30000/60000 samples\n",
      "Processed 40000/60000 samples\n",
      "Processed 50000/60000 samples\n",
      "Processed 60000/60000 samples\n",
      "Finished training epoch. Average Loss: 0.9748 \n",
      "\n",
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 0.86\n",
      "Accuracy (# of correct guesses): 7026.0 / 10000 ( 70.26 %)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_one_epoch(learning_rate=0.003):\n",
    "    print(\"Training for one epoch over the training dataset...\")\n",
    "    total_loss = 0.0\n",
    "    num_samples = x_train.shape[0]\n",
    "    \n",
    "    # Loop through every sample in the training set\n",
    "    for i in range(num_samples):\n",
    "        loss = train_one_sample(x_train[i], y_train[i], learning_rate)\n",
    "        total_loss += loss\n",
    "        if (i+1) % 10000 == 0:\n",
    "            print(f\"Processed {i+1}/{num_samples} samples\")\n",
    "    \n",
    "    avg_loss = total_loss / num_samples\n",
    "    print(\"Finished training epoch. Average Loss:\", np.round(avg_loss, decimals=4), \"\\n\")\n",
    "\n",
    "feed_forward_test_data()\n",
    "\n",
    "def test_and_train():\n",
    "  train_one_epoch()\n",
    "  feed_forward_test_data()\n",
    "\n",
    "for i in range(3): \n",
    "  test_and_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKzEn_lyCAIe"
   },
   "source": [
    "\n",
    "That's it! \n",
    "\n",
    "Your code is probably very time- and memory-inefficient; that's ok. There is a ton of optimization under the hood in professional deep learning frameworks which we won't get into.\n",
    "\n",
    "If everything is working well, you should be able to raise the accuracy from ~10% to ~70% accuracy after 3 epochs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
