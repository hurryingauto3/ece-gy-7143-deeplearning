{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"945d105ead994f76befbd0e82dc7795c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6d8c9b2eff24e10910c3ac09eb234b1","IPY_MODEL_c8ac90bb59f949d08f903fbd1a94e5a8","IPY_MODEL_f016b72d72d94010a0f6788251ed2842"],"layout":"IPY_MODEL_a2dc9fc7a2ff432db8fbecfa0b691c5d"}},"f6d8c9b2eff24e10910c3ac09eb234b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf2c736851f64d6d8ca5df2e6629df39","placeholder":"​","style":"IPY_MODEL_ad8cea3a91ea463cb09818b9526a8d51","value":"Map: 100%"}},"c8ac90bb59f949d08f903fbd1a94e5a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af827b8ed9824e89b13f570c760277b0","max":7600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bd4c455124949af8851895001885897","value":7600}},"f016b72d72d94010a0f6788251ed2842":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f16e4c9098e45a2978753774e5e7f44","placeholder":"​","style":"IPY_MODEL_36f1573e3efe48ebb54ede3764b4e867","value":" 7600/7600 [00:01&lt;00:00, 5687.18 examples/s]"}},"a2dc9fc7a2ff432db8fbecfa0b691c5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf2c736851f64d6d8ca5df2e6629df39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad8cea3a91ea463cb09818b9526a8d51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af827b8ed9824e89b13f570c760277b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd4c455124949af8851895001885897":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f16e4c9098e45a2978753774e5e7f44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36f1573e3efe48ebb54ede3764b4e867":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c1823dd4d3e41d0913e5708b7c7892e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_168d0e769da64fb1b1948d2e2c2a41f9","IPY_MODEL_69fdb4c78f174696b1b34b1d0e51ff0e","IPY_MODEL_540202ceae8f47c9ae590b4c71975be0"],"layout":"IPY_MODEL_569083f6397c456581d2c6ad0137461e"}},"168d0e769da64fb1b1948d2e2c2a41f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb89eda85ef44a0593772105b83c1452","placeholder":"​","style":"IPY_MODEL_c1b8ac865a15430596dfa74e3bd63d4f","value":"model.safetensors: 100%"}},"69fdb4c78f174696b1b34b1d0e51ff0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dec70835e8274587a6b41aa2a4f9e7cf","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5da6634312534dbcaa66a349092507cd","value":498818054}},"540202ceae8f47c9ae590b4c71975be0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e591897c9964d18ac6419ed25b8360b","placeholder":"​","style":"IPY_MODEL_26ac8e7a161c4be39130e202c65643e2","value":" 499M/499M [00:06&lt;00:00, 99.0MB/s]"}},"569083f6397c456581d2c6ad0137461e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb89eda85ef44a0593772105b83c1452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1b8ac865a15430596dfa74e3bd63d4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dec70835e8274587a6b41aa2a4f9e7cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5da6634312534dbcaa66a349092507cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e591897c9964d18ac6419ed25b8360b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26ac8e7a161c4be39130e202c65643e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c676d3d98aa84b8492e1a74ef8ba7ec2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1a52f19f7764667a4738180f82817fc","IPY_MODEL_5397426e71d94b8e8703ac2901de70a4","IPY_MODEL_f4c4666f185c405b89dd09c98a6480ab"],"layout":"IPY_MODEL_2dcc4ea65d52488a8551bac7c90a536d"}},"c1a52f19f7764667a4738180f82817fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a237a616f03d4f28b66a34b6dd0bde9a","placeholder":"​","style":"IPY_MODEL_da5ea0abcecb4fdab4340436589fa0fe","value":"Downloading builder script: 100%"}},"5397426e71d94b8e8703ac2901de70a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1403e13943d4504b47cb528d9ea8915","max":4203,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f4b9f0c60784614871f2e09c79d102a","value":4203}},"f4c4666f185c405b89dd09c98a6480ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a3002edb0464d61a875aa5b95458534","placeholder":"​","style":"IPY_MODEL_75e19d26df8c4b179af6043e4b9e9960","value":" 4.20k/4.20k [00:00&lt;00:00, 229kB/s]"}},"2dcc4ea65d52488a8551bac7c90a536d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a237a616f03d4f28b66a34b6dd0bde9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da5ea0abcecb4fdab4340436589fa0fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1403e13943d4504b47cb528d9ea8915":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f4b9f0c60784614871f2e09c79d102a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a3002edb0464d61a875aa5b95458534":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75e19d26df8c4b179af6043e4b9e9960":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets evaluate","metadata":{"id":"YQkw3v907vn6","outputId":"c9f5588d-e1bc-456a-de53-cc678024ec6b","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:18:58.773088Z","iopub.execute_input":"2025-04-15T00:18:58.773642Z","iopub.status.idle":"2025-04-15T00:19:01.858105Z","shell.execute_reply.started":"2025-04-15T00:18:58.773617Z","shell.execute_reply":"2025-04-15T00:19:01.857178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\n\n# Define Kaggle credentials\nkaggle_creds = {\n    \"username\": \"hurryingauto3\",\n    \"key\": \"17e33c07cfd0993aecbc770b33c7054e\"\n}\n\n# Ensure the Kaggle config directory exists\nos.makedirs(os.path.expanduser(\"~/.config/kaggle/\"), exist_ok=True)\n\n# Write credentials to kaggle.json\nwith open(os.path.expanduser(\"~/.config/kaggle/kaggle.json\"), \"w\") as f:\n    json.dump(kaggle_creds, f)\n\n# Set correct permissions\nos.chmod(os.path.expanduser(\"~/.config/kaggle/kaggle.json\"), 0o600)\n\n# Remove the \"data/\" directory if it exists\nos.system(\"rm -rf data/\")","metadata":{"id":"WXlsSvXjqrpb","outputId":"09bc76f2-b818-4757-c8f9-a7da35f48e0f","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:19:01.859797Z","iopub.execute_input":"2025-04-15T00:19:01.860039Z","iopub.status.idle":"2025-04-15T00:19:01.870447Z","shell.execute_reply.started":"2025-04-15T00:19:01.860011Z","shell.execute_reply":"2025-04-15T00:19:01.869889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport os\nimport pickle\nimport zipfile\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom kaggle.api.kaggle_api_extended import KaggleApi\n\ntry:\n    from datasets import Dataset as HFDataset # Use an alias to avoid conflict with torch.utils.data.Dataset\n    from datasets import load_dataset\nexcept ImportError:\n    print(\"Please install the 'datasets' library: pip install datasets\")\n    HFDataset = None\n","metadata":{"id":"W8PGM0LwApq0","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:19:01.871183Z","iopub.execute_input":"2025-04-15T00:19:01.871814Z","iopub.status.idle":"2025-04-15T00:19:25.357853Z","shell.execute_reply.started":"2025-04-15T00:19:01.871796Z","shell.execute_reply":"2025-04-15T00:19:25.356830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- Custom Dataset for the Competition Test File ---\nclass AGNewsTestDataset(Dataset):\n    \"\"\"\n    Custom dataset for AGNEWS competition test text data.\n    Handles test data stored as a pickled Hugging Face Dataset object.\n\n    Args:\n        pkl_file (str): Path to the pickle file containing the test data (expected as HF Dataset).\n        tokenizer (callable): Tokenizer instance (e.g., from Hugging Face)\n        max_length (int): Maximum sequence length for tokenization.\n        text_column (str): The name of the column containing the text in the pickled Dataset. Defaults to 'text'.\n    \"\"\"\n    def __init__(self, pkl_file, tokenizer, max_length=512, text_column=\"text\"):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.text_column = text_column\n        self.texts = [] # Initialize as empty list\n\n        try:\n            with open(pkl_file, 'rb') as f:\n                loaded_object = pickle.load(f)\n\n            # --- Check if the loaded object is a Hugging Face Dataset ---\n            if HFDataset is not None and isinstance(loaded_object, HFDataset):\n                print(f\"Pickle file contained a Hugging Face Dataset object.\")\n                # Check if the expected text column exists\n                if self.text_column in loaded_object.column_names:\n                    # Extract the text column into a list\n                    self.texts = loaded_object[self.text_column]\n                    print(f\"Successfully extracted '{self.text_column}' column ({len(self.texts)} items).\")\n                else:\n                    raise ValueError(f\"Loaded Dataset object does not contain the expected text column '{self.text_column}'. \"\n                                     f\"Available columns: {loaded_object.column_names}\")\n            # --- Fallback: Check if it's a list (original assumption) ---\n            elif isinstance(loaded_object, list):\n                 print(\"Pickle file contained a standard Python list.\")\n                 self.texts = loaded_object\n            # --- Fallback: Check if it's a dictionary (previous check) ---\n            elif isinstance(loaded_object, dict):\n                 print(\"Pickle file contained a standard Python dict.\")\n                 possible_keys = ['text', 'data', 'description'] # Add other likely keys if needed\n                 data_key = next((k for k in possible_keys if k in loaded_object), None)\n                 if data_key and isinstance(loaded_object[data_key], list):\n                     print(f\"Assuming text data is under key '{data_key}'.\")\n                     self.texts = loaded_object[data_key]\n                 else:\n                     raise ValueError(f\"Could not find a list of texts in pkl dictionary. Keys found: {list(loaded_object.keys())}\")\n            # --- If none of the above ---\n            else:\n                 raise TypeError(f\"Unsupported data type loaded from pickle file: {type(loaded_object)}. \"\n                                 \"Expected Hugging Face Dataset, list, or dict containing a list.\")\n\n            # Final check if texts were actually loaded\n            if not self.texts:\n                 raise ValueError(f\"Failed to load any text data from the pickle file: {pkl_file}\")\n\n\n        except FileNotFoundError:\n            print(f\"Error: Test pickle file not found at {pkl_file}\")\n            raise\n        except Exception as e:\n            print(f\"Error loading or processing pickle file {pkl_file}: {e}\")\n            raise\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        # Ensure text is a string (might be redundant if extracted from HF Dataset, but safe)\n        if not isinstance(text, str):\n            text = str(text)\n\n        # Tokenize the text\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding=False, # Padding will be handled by the collator\n            max_length=self.max_length,\n            return_tensors=None, # Return python lists/ints, collator handles tensor conversion\n        )\n\n        # Return the tokenized inputs and the original index for submission mapping\n        # Remove 'token_type_ids' if your model doesn't use them (like RoBERTa)\n        item = {k: v for k, v in encoding.items() if k != 'token_type_ids'}\n        item['index'] = index # Include original index\n\n        return item\n\n# --- Data Module for AGNEWS ---\nclass AGNewsDataModule:\n    \"\"\"\n    Data module for AGNEWS dataset (train/val from Hugging Face, test from competition file).\n\n    Args:\n        model_name_or_path (str): Identifier for the tokenizer (e.g., \"roberta-base\").\n        data_dir (str): Directory to potentially store data (less critical when using `datasets`).\n        competition_name (str): Name of the Kaggle competition for downloading test data.\n        batch_size (int): Training batch size.\n        test_batch_size (int): Testing/Validation batch size.\n        num_workers (int): Number of workers for data loading.\n        max_seq_length (int): Maximum sequence length for tokenizer.\n        val_split_percentage (float): Percentage of training data to use for validation (0 to disable).\n    \"\"\"\n    def __init__(self,\n                 model_name_or_path=\"roberta-base\",\n                 data_dir=\"./data_agnews\",\n                 competition_name=\"deep-learning-spring-2025-project-2\", # UPDATE IF NEEDED\n                 batch_size=16,\n                 test_batch_size=32,\n                 num_workers=2,\n                 max_seq_length=512,\n                 val_split_percentage=0.1): # Use 10% of train for validation\n\n        self.model_name_or_path = model_name_or_path\n        self.data_dir = data_dir\n        self.competition_name = competition_name\n        self.batch_size = batch_size\n        self.test_batch_size = test_batch_size\n        self.num_workers = num_workers\n        self.max_seq_length = max_seq_length\n        self.val_split_percentage = val_split_percentage\n\n        # Paths for competition data\n        self.competition_path = os.path.join(self.data_dir, self.competition_name)\n        self.zip_path = os.path.join(self.competition_path, f\"{self.competition_name}.zip\")\n        self.test_pkl = os.path.join(self.competition_path, \"test_unlabelled.pkl\") # Correct filename\n\n        # Initialize tokenizer and data collator\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)\n        # Data collator handles dynamic padding within each batch\n        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n\n        self.train_dataset = None\n        self.val_dataset = None\n        self.predict_dataset = None\n\n    def _tokenize_function(self, examples):\n        # Tokenize the text field. AGNEWS uses 'text'.\n        # Padding is false here; collator handles it later.\n        return self.tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding=False,\n            max_length=self.max_seq_length\n        )\n\n    def prepare_data(self):\n        \"\"\"Downloads competition data if needed.\"\"\"\n        # Download standard AGNEWS train/test via `datasets` library automatically on first use.\n        print(\"Checking/downloading AGNEWS dataset from Hugging Face...\")\n        load_dataset(\"ag_news\", cache_dir=os.path.join(self.data_dir, \"hf_cache\"))\n        print(\"Checking/downloading competition test data...\")\n        self.download_competition_data()\n\n    def setup(self, stage=None):\n        \"\"\"Loads and preprocesses datasets.\"\"\"\n        # Load AGNEWS dataset\n        dataset = load_dataset(\"ag_news\", cache_dir=os.path.join(self.data_dir, \"hf_cache\"))\n\n        # Tokenize dataset\n        tokenized_dataset = dataset.map(self._tokenize_function, batched=True)\n\n        # Remove original text column, select necessary columns\n        tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n        # Rename 'label' to 'labels' if required by the model/trainer framework\n        # tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n        tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n        if stage == \"fit\" or stage is None:\n            ag_train_data = tokenized_dataset[\"train\"]\n            if self.val_split_percentage > 0:\n                split = ag_train_data.train_test_split(test_size=self.val_split_percentage)\n                self.train_dataset = split['train']\n                self.val_dataset = split['test']\n                print(f\"Using {len(self.train_dataset)} samples for training, {len(self.val_dataset)} for validation.\")\n            else:\n                # Use standard AGNEWS test set as validation if no split % is given\n                self.train_dataset = ag_train_data\n                self.val_dataset = tokenized_dataset[\"test\"]\n                print(f\"Using {len(self.train_dataset)} samples for training, {len(self.val_dataset)} (standard test set) for validation.\")\n\n\n        if stage == \"validate\" or stage is None:\n             if self.val_dataset is None: # If setup wasn't called with 'fit'\n                 # Load validation data (standard AGNEWS test set)\n                 self.val_dataset = tokenized_dataset[\"test\"]\n                 print(f\"Loaded {len(self.val_dataset)} (standard test set) for validation.\")\n\n\n        if stage == \"test\" or stage is None:\n            # Setup competition test dataset\n             print(f\"Setting up competition test dataset from: {self.test_pkl}\")\n             self.predict_dataset = AGNewsTestDataset(\n                 self.test_pkl,\n                 self.tokenizer,\n                 self.max_seq_length\n             )\n             print(f\"Loaded {len(self.predict_dataset)} samples for competition prediction.\")\n\n\n    def get_train_loader(self):\n        if not self.train_dataset:\n            self.setup(\"fit\")\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            collate_fn=self.data_collator # Use collator for dynamic padding\n        )\n\n    def get_val_loader(self):\n        if not self.val_dataset:\n            self.setup(\"validate\") # Or 'fit' if you always run setup completely\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.test_batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            collate_fn=self.data_collator # Use collator for dynamic padding\n        )\n\n    def get_competition_test_loader(self):\n        \"\"\"Gets the DataLoader for the competition's unlabelled test set.\"\"\"\n        if not self.predict_dataset:\n            self.setup(\"test\")\n        return DataLoader(\n            self.predict_dataset,\n            batch_size=self.test_batch_size,\n            shuffle=False, # Important: Keep order for submission\n            num_workers=self.num_workers,\n            collate_fn=self.data_collator # Use collator for dynamic padding - it handles dicts well\n        )\n\n    def download_competition_data(self):\n        \"\"\"Downloads and extracts competition test data using Kaggle API.\"\"\"\n        if not os.path.exists(self.test_pkl):\n            print(f\"Competition test file not found at {self.test_pkl}. Attempting download...\")\n            os.makedirs(self.competition_path, exist_ok=True)\n            try:\n                from kaggle.api.kaggle_api_extended import KaggleApi\n                api = KaggleApi()\n                api.authenticate() # Make sure kaggle.json is set up\n                api.competition_download_files(self.competition_name, path=self.competition_path)\n\n                if os.path.exists(self.zip_path):\n                    print(f\"Extracting {self.zip_path}...\")\n                    with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n                        zip_ref.extractall(self.competition_path)\n                    os.remove(self.zip_path) # Clean up the zip file\n                    print(\"Extraction complete.\")\n                else:\n                     print(f\"Warning: Zip file {self.zip_path} not found after download attempt.\")\n\n            except ImportError:\n                print(\"Warning: 'kaggle' library not found. Cannot download competition data automatically.\")\n                print(\"Please download the 'test_unlabelled.pkl' manually from the Kaggle competition page\")\n                print(f\"and place it in: {self.competition_path}\")\n            except Exception as e:\n                print(f\"An error occurred during Kaggle download/extraction: {e}\")\n                print(\"Please check your Kaggle API setup and competition name.\")\n\n        if not os.path.exists(self.test_pkl):\n            # Raise error only after attempting download\n            raise FileNotFoundError(\n                f\"Competition test file '{os.path.basename(self.test_pkl)}' not found in '{self.competition_path}'. \"\n                \"Please ensure it is downloaded and extracted correctly.\"\n            )\n        else:\n            print(f\"Competition test file found: {self.test_pkl}\")\n","metadata":{"id":"i-y0FkhJp-kG","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:19:25.360455Z","iopub.execute_input":"2025-04-15T00:19:25.361440Z","iopub.status.idle":"2025-04-15T00:19:25.396747Z","shell.execute_reply.started":"2025-04-15T00:19:25.361407Z","shell.execute_reply":"2025-04-15T00:19:25.395754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example configuration\nMODEL_ID = \"roberta-base\"\nCOMPETITION_ID = \"deep-learning-spring-2025-project-2\" # Double-check this ID\nDATA_DIR = \"./agnews_data\"\nBATCH_SIZE = 8 # Small batch size for demo\nTEST_BATCH_SIZE = 16\nMAX_LEN = 128 # Shorter length for faster demo processing\n\n# Instantiate the data module\ndata_module = AGNewsDataModule(\n    model_name_or_path=MODEL_ID,\n    data_dir=DATA_DIR,\n    competition_name=COMPETITION_ID,\n    batch_size=BATCH_SIZE,\n    test_batch_size=TEST_BATCH_SIZE,\n    max_seq_length=MAX_LEN,\n    num_workers=0 # Set to 0 for easier debugging in __main__\n)\n\ndata_module.prepare_data() # Downloads HF data and competition data if needed\n\ndata_module.setup() # Sets up train, val, and test","metadata":{"id":"VBzMLw2oxvXV","outputId":"c68f6fa7-30f5-42f4-ddf7-1bcff2847642","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:19:25.397803Z","iopub.execute_input":"2025-04-15T00:19:25.398087Z","iopub.status.idle":"2025-04-15T00:19:57.859079Z","shell.execute_reply.started":"2025-04-15T00:19:25.398063Z","shell.execute_reply":"2025-04-15T00:19:57.858317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# --- Visualize Training Samples ---\nprint(\"\\n--- Train Set Samples (First 5) ---\")\ntry:\n    if data_module.train_dataset:\n        # Select the first 5 samples directly from the Hugging Face dataset\n        train_samples = data_module.train_dataset.select(range(min(5, len(data_module.train_dataset))))\n\n        train_data_for_df = []\n        for sample in train_samples:\n            text = data_module.tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n            # Ensure 'label' exists, otherwise use a placeholder like None or -1\n            label = sample.get('label', None)\n            if isinstance(label, torch.Tensor):\n                  label = label.item() # Convert tensor to Python number\n            train_data_for_df.append({'Decoded Text': text, 'Label': label})\n\n        train_df = pd.DataFrame(train_data_for_df)\n        print(train_df)\n    else:\n        print(\"Train dataset not loaded or empty.\")\nexcept Exception as e:\n    print(f\"Error displaying train samples: {e}\")","metadata":{"id":"kczrZ0BWqy1H","outputId":"8ffa5f1a-0793-4ae1-8813-200b1a12e339","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:19:57.860165Z","iopub.execute_input":"2025-04-15T00:19:57.860412Z","iopub.status.idle":"2025-04-15T00:19:57.915879Z","shell.execute_reply.started":"2025-04-15T00:19:57.860387Z","shell.execute_reply":"2025-04-15T00:19:57.915238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Visualize Validation Samples ---\nprint(\"\\n--- Validation Set Samples (First 5) ---\")\ntry:\n    if data_module.val_dataset:\n        # Select the first 5 samples directly from the Hugging Face dataset\n        val_samples = data_module.val_dataset.select(range(min(5, len(data_module.val_dataset))))\n\n        val_data_for_df = []\n        for sample in val_samples:\n            text = data_module.tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n            # Ensure 'label' exists, otherwise use a placeholder like None or -1\n            label = sample.get('label', None)\n            if isinstance(label, torch.Tensor):\n                  label = label.item() # Convert tensor to Python number\n            val_data_for_df.append({'Decoded Text': text, 'Label': label})\n\n        val_df = pd.DataFrame(val_data_for_df)\n        print(val_df)\n    else:\n        print(\"Validation dataset not loaded or empty.\")\nexcept Exception as e:\n    print(f\"Error displaying validation samples: {e}\")","metadata":{"id":"C5djOy67q50y","outputId":"48dc22e6-172f-4b39-97e1-d4a293471e06","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:19:57.916718Z","iopub.execute_input":"2025-04-15T00:19:57.916973Z","iopub.status.idle":"2025-04-15T00:19:57.933101Z","shell.execute_reply.started":"2025-04-15T00:19:57.916953Z","shell.execute_reply":"2025-04-15T00:19:57.932369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Visualize Competition Test Samples ---\nprint(\"\\n--- Competition Test Set Samples (First 5) ---\")\ntry:\n    if data_module.predict_dataset:\n        test_data_for_df = []\n        # Iterate through the custom dataset using __getitem__\n        num_samples_to_show = min(5, len(data_module.predict_dataset))\n        for i in range(num_samples_to_show):\n            sample = data_module.predict_dataset[i] # Fetches the dictionary item\n            text = data_module.tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n            # Ensure 'index' exists, otherwise use a placeholder like None or -1\n            original_index = sample.get('index', None)\n            test_data_for_df.append({'Decoded Text': text, 'Original Index': original_index})\n\n        test_df = pd.DataFrame(test_data_for_df)\n        print(test_df)\n    else:\n        print(\"Competition test dataset not loaded or empty.\")\nexcept Exception as e:\n    print(f\"Error displaying competition test samples: {e}\")","metadata":{"id":"0rNQ9-6Eq-gk","outputId":"6b052198-c3c9-4976-f10a-2bd78c317abd","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:19:57.934018Z","iopub.execute_input":"2025-04-15T00:19:57.934286Z","iopub.status.idle":"2025-04-15T00:19:57.947271Z","shell.execute_reply.started":"2025-04-15T00:19:57.934265Z","shell.execute_reply":"2025-04-15T00:19:57.946658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport evaluate # Hugging Face evaluate library\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding # Make sure this is imported\n)\nfrom peft import LoraConfig, TaskType, get_peft_model\n\n","metadata":{"id":"htBNx68lqdrr","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:19:57.948062Z","iopub.execute_input":"2025-04-15T00:19:57.948293Z","iopub.status.idle":"2025-04-15T00:20:05.584197Z","shell.execute_reply.started":"2025-04-15T00:19:57.948276Z","shell.execute_reply":"2025-04-15T00:20:05.583321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"nH5s2T44wRIR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bse RoBERTa Model Import","metadata":{"id":"RuSShyAL8SVp"}},{"cell_type":"code","source":"# Assume AGNewsDataModule is defined in another file or earlier in the script\n# from your_data_module_file import AGNewsDataModule\n\n# --- Configuration ---\nmodel_name = \"roberta-base\"\nnum_labels = 4\nlora_r = 8\nlora_alpha = 16\nlora_dropout = 0.1\ntarget_modules = [\"query\", \"value\"]\noutput_dir = \"./results/roberta-lora-agnews\" # Directory to save checkpoints and logs\ntraining_log_dir = \"./logs/roberta-lora-agnews\" # Directory for TensorBoard/logging\nadapter_save_dir = \"./trained_adapters/roberta-lora-agnews\" # Directory to save final adapter\n\n# --- Hyperparameters for Training ---\n# These are crucial and require tuning!\nlearning_rate = 2e-4 # LoRA might tolerate/need higher LR than full finetuning\ntrain_batch_size = 16 # Adjust based on GPU memory\neval_batch_size = 32  # Adjust based on GPU memory\nnum_train_epochs = 3  # Start with a few epochs, increase as needed\nweight_decay = 0.01\nwarmup_ratio = 0.1 # Percentage of steps for learning rate warmup\n\n","metadata":{"id":"c77caa05","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:05.586956Z","iopub.execute_input":"2025-04-15T00:20:05.587581Z","iopub.status.idle":"2025-04-15T00:20:05.592594Z","shell.execute_reply.started":"2025-04-15T00:20:05.587520Z","shell.execute_reply":"2025-04-15T00:20:05.591786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Load Base Model (as you did) ---\nprint(f\"Loading base model '{model_name}'...\")\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels\n)","metadata":{"id":"dfoSRkq9wTa3","outputId":"f3c3ab22-c527-4987-a1d8-723faedb03b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:05.593658Z","iopub.execute_input":"2025-04-15T00:20:05.593956Z","iopub.status.idle":"2025-04-15T00:20:08.323026Z","shell.execute_reply.started":"2025-04-15T00:20:05.593930Z","shell.execute_reply":"2025-04-15T00:20:08.322301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Define LoRA Config (as you did) ---\nprint(\"Defining LoRA configuration...\")\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=lora_r,\n    lora_alpha=lora_alpha,\n    target_modules=target_modules,\n    lora_dropout=lora_dropout,\n    bias=\"none\",\n)\n","metadata":{"id":"mBfFThOjwfqI","outputId":"5070a73b-7cef-4ad2-c5a3-506de067b618","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:08.323966Z","iopub.execute_input":"2025-04-15T00:20:08.324229Z","iopub.status.idle":"2025-04-15T00:20:08.328365Z","shell.execute_reply.started":"2025-04-15T00:20:08.324207Z","shell.execute_reply":"2025-04-15T00:20:08.327832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Apply LoRA to the model (as you did) ---\nprint(\"Applying LoRA adapter to the model...\")\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters() # Verify parameter count is low\n","metadata":{"id":"5VcoHYepwXKk","outputId":"64e5aa3f-6b05-4475-adce-570900b03d13","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:08.329140Z","iopub.execute_input":"2025-04-15T00:20:08.329395Z","iopub.status.idle":"2025-04-15T00:20:08.401975Z","shell.execute_reply.started":"2025-04-15T00:20:08.329372Z","shell.execute_reply":"2025-04-15T00:20:08.401389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = data_module.get_train_loader().dataset # Get the underlying Dataset object\nval_dataset = data_module.get_val_loader().dataset     # Get the underlying Dataset object\ntokenizer = data_module.tokenizer                     # Get the tokenizer\ndata_collator = data_module.data_collator             # Get the data collator","metadata":{"id":"pVhzY7DawXTN","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:08.402693Z","iopub.execute_input":"2025-04-15T00:20:08.403242Z","iopub.status.idle":"2025-04-15T00:20:08.407020Z","shell.execute_reply.started":"2025-04-15T00:20:08.403220Z","shell.execute_reply":"2025-04-15T00:20:08.406273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")","metadata":{"id":"WC_gG7Eewqqw","outputId":"41aae974-7ef4-4345-e47d-2fe5eefde9a4","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:08.407945Z","iopub.execute_input":"2025-04-15T00:20:08.408191Z","iopub.status.idle":"2025-04-15T00:20:08.422197Z","shell.execute_reply.started":"2025-04-15T00:20:08.408169Z","shell.execute_reply":"2025-04-15T00:20:08.421592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Define Compute Metrics Function ---\n# Load the accuracy metric\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on evaluation predictions.\"\"\"\n    predictions, labels = eval_pred\n    # 'predictions' are logits, convert to predicted class index\n    preds = np.argmax(predictions, axis=1)\n    # Calculate accuracy\n    acc = accuracy_metric.compute(predictions=preds, references=labels)\n    return {\"accuracy\": acc[\"accuracy\"]}\n","metadata":{"id":"UzAGnMFQwXY7","outputId":"a7ebfdf5-30fd-4569-9df9-5bd3ad9fbcc3","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:08.422791Z","iopub.execute_input":"2025-04-15T00:20:08.422966Z","iopub.status.idle":"2025-04-15T00:20:08.991251Z","shell.execute_reply.started":"2025-04-15T00:20:08.422953Z","shell.execute_reply":"2025-04-15T00:20:08.990782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6. Define Training Arguments ---\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    learning_rate=learning_rate,\n    per_device_train_batch_size=train_batch_size,\n    per_device_eval_batch_size=eval_batch_size,\n    weight_decay=weight_decay,\n    warmup_ratio=warmup_ratio,\n\n    # Evaluation and Saving Strategy\n    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n    save_strategy=\"epoch\",       # Save checkpoint at the end of each epoch\n    load_best_model_at_end=True, # Load the best model found during training\n    metric_for_best_model=\"accuracy\", # Use accuracy to determine the best model\n    greater_is_better=True,      # Higher accuracy is better\n\n    # Logging\n    logging_dir=training_log_dir,\n    logging_strategy=\"steps\",\n    logging_steps=50,            # Log metrics every 50 steps\n\n    # Other potentially useful args\n    # fp16=True,                 # Enable mixed precision training if GPU supports it (requires accelerate)\n    # gradient_accumulation_steps=2, # If batch size needs to be effectively larger than fits in memory\n    report_to=\"tensorboard\",     # Report logs to TensorBoard (can also use \"wandb\")\n    save_total_limit=2,          # Keep only the last 2 checkpoints + the best one\n    # push_to_hub=False,         # Set to True to push model to Hugging Face Hub\n)","metadata":{"id":"6MMBr27LwXb4","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:08.991943Z","iopub.execute_input":"2025-04-15T00:20:08.992205Z","iopub.status.idle":"2025-04-15T00:20:09.019266Z","shell.execute_reply.started":"2025-04-15T00:20:08.992182Z","shell.execute_reply":"2025-04-15T00:20:09.018787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 7. Initialize Trainer ---\ntrainer = Trainer(\n    model=model,                         # The PEFT model\n    args=training_args,                  # Training arguments\n    train_dataset=train_dataset,         # Training dataset\n    eval_dataset=val_dataset,            # Validation dataset\n    tokenizer=tokenizer,                 # Tokenizer (needed for padding/saving)\n    data_collator=data_collator,         # Data collator for dynamic padding\n    compute_metrics=compute_metrics,     # Function to compute metrics\n)\n","metadata":{"id":"sXmDmEkawXfL","outputId":"1523e2d1-ca8f-4380-e453-eeb4ffa9a61e","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:09.019909Z","iopub.execute_input":"2025-04-15T00:20:09.020076Z","iopub.status.idle":"2025-04-15T00:20:09.605934Z","shell.execute_reply.started":"2025-04-15T00:20:09.020064Z","shell.execute_reply":"2025-04-15T00:20:09.605306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 8. Start Training ---\nprint(\"\\nStarting Training...\")\ntrain_result = trainer.train()\n# --- 9. Save Training Stats and Final Adapter ---\nprint(\"\\nTraining finished. Saving metrics and final adapter...\")\n# Saves metrics like loss, learning rate, epoch, etc. to json file\ntrainer.log_metrics(\"train\", train_result.metrics)\ntrainer.save_metrics(\"train\", train_result.metrics)\n","metadata":{"id":"vBfHlfaZwXib","outputId":"f4a6b747-2684-4063-8b5d-24b93691f784","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:20:09.606660Z","iopub.execute_input":"2025-04-15T00:20:09.606919Z","iopub.status.idle":"2025-04-15T01:00:36.863223Z","shell.execute_reply.started":"2025-04-15T00:20:09.606900Z","shell.execute_reply":"2025-04-15T01:00:36.862423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained LoRA adapter weights explicitly\n# This saves only the adapter weights, which is the goal of PEFT\nmodel.save_pretrained(adapter_save_dir)\n# You might also want to save the tokenizer with the adapter for easy loading later\ntokenizer.save_pretrained(adapter_save_dir)\nprint(f\"LoRA adapter weights saved to: {adapter_save_dir}\")","metadata":{"id":"QRx5NzMAxEY4","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:00:36.864008Z","iopub.execute_input":"2025-04-15T01:00:36.864259Z","iopub.status.idle":"2025-04-15T01:00:37.110332Z","shell.execute_reply.started":"2025-04-15T01:00:36.864242Z","shell.execute_reply":"2025-04-15T01:00:37.109759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 10. Evaluate Final Model (Optional but recommended) ---\nprint(\"\\nEvaluating the best model on the validation set...\")\neval_metrics = trainer.evaluate(eval_dataset=val_dataset) # Use the same validation set\ntrainer.log_metrics(\"eval\", eval_metrics)\ntrainer.save_metrics(\"eval\", eval_metrics)\nprint(f\"Final Evaluation Metrics: {eval_metrics}\")\n\nprint(\"\\nLoRA Training Setup Complete.\")","metadata":{"id":"1SKBrfk_wXlS","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:00:37.111052Z","iopub.execute_input":"2025-04-15T01:00:37.111268Z","iopub.status.idle":"2025-04-15T01:01:14.917886Z","shell.execute_reply.started":"2025-04-15T01:00:37.111252Z","shell.execute_reply":"2025-04-15T01:01:14.917382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport os # Import os for path joining if reading metrics from file\n# import json # Import json if reading metrics from file\nfrom tqdm.auto import tqdm # For progress bar\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom peft import PeftModel, PeftConfig\nfrom torch.utils.data import DataLoader\n# Assume AGNewsDataModule and AGNewsTestDataset are defined\n# from your_data_module_file import AGNewsDataModule, AGNewsTestDataset\n\n# --- Configuration ---\nbase_model_name = \"roberta-base\"\nadapter_path = \"./trained_adapters/roberta-lora-agnews\" # Directory where you saved the adapter\nnum_labels = 4\neval_batch_size = 32\nmax_seq_length = 128\n\n# --- Determine Accuracy for Filename ---\n\nfinal_accuracy = eval_metrics['eval_accuracy']\nprint(f\"Using final accuracy for filename: {final_accuracy:.4f}\")\n\n# --- Set Output Filename ---\noutput_csv_path = f\"submission_acc_{final_accuracy:.4f}.csv\"\nprint(f\"Output file will be saved as: {output_csv_path}\")\n\n# --- Device Setup ---\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU device.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:20:14.707654Z","iopub.execute_input":"2025-04-15T01:20:14.708308Z","iopub.status.idle":"2025-04-15T01:20:14.714332Z","shell.execute_reply.started":"2025-04-15T01:20:14.708282Z","shell.execute_reply":"2025-04-15T01:20:14.713723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Load Tokenizer ---\nprint(f\"Loading tokenizer from '{base_model_name}'...\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n# --- 2. Load Base Model ---\nprint(f\"Loading base model '{base_model_name}'...\")\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    base_model_name,\n    num_labels=num_labels,\n    return_dict=True\n)\n\n# --- 3. Load LoRA Adapter Weights ---\nprint(f\"Loading LoRA adapter weights from '{adapter_path}'...\")\ntry:\n    model = PeftModel.from_pretrained(base_model, adapter_path)\n    print(\"Successfully loaded LoRA adapter.\")\nexcept Exception as e:\n    print(f\"Error loading PEFT model from {adapter_path}: {e}\")\n    raise\n\n# --- 4. Prepare Model for Inference ---\nmodel = model.to(device)\nmodel.eval()\nprint(\"Model moved to device and set to evaluation mode.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:20:43.756427Z","iopub.execute_input":"2025-04-15T01:20:43.757137Z","iopub.status.idle":"2025-04-15T01:20:44.473164Z","shell.execute_reply.started":"2025-04-15T01:20:43.757106Z","shell.execute_reply":"2025-04-15T01:20:44.472590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncompetition_loader = data_module.get_competition_test_loader()\nprint(f\"Test data loaded. Number of batches: {len(competition_loader)}\")\n\n# --- 6. Run Inference ---\nall_predictions = []\nall_indices = []\n\nprint(\"\\nStarting prediction loop...\")\nwith torch.no_grad():\n    for batch in tqdm(competition_loader, desc=\"Predicting\"):\n        model_inputs = {\n            k: v.to(device) for k, v in batch.items()\n            if k in tokenizer.model_input_names\n        }\n        indices = batch['index'].cpu().numpy()\n        outputs = model(**model_inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        predictions_np = predictions.cpu().numpy()\n        all_predictions.extend(predictions_np)\n        all_indices.extend(indices)\nprint(\"Prediction loop finished.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:21:28.441828Z","iopub.execute_input":"2025-04-15T01:21:28.442088Z","iopub.status.idle":"2025-04-15T01:21:57.905855Z","shell.execute_reply.started":"2025-04-15T01:21:28.442069Z","shell.execute_reply":"2025-04-15T01:21:57.905157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 7. Create DataFrame and Save CSV ---\nsubmission_df = pd.DataFrame({\n    'ID': all_indices,\n    'Label': all_predictions\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:30:53.411431Z","iopub.execute_input":"2025-04-15T01:30:53.412125Z","iopub.status.idle":"2025-04-15T01:30:53.446262Z","shell.execute_reply.started":"2025-04-15T01:30:53.412102Z","shell.execute_reply":"2025-04-15T01:30:53.445599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge the two dataframes based on the index columns\n# We use 'Original Index' from test_df and 'index' from submission_df\n# 'how=inner' ensures only matching indices are kept (should be all of them)\nview_df = pd.merge(\n    test_df,\n    submission_df,\n    left_on='Original Index', # Key column in the left DataFrame (test_df)\n    right_on='ID',         # Key column in the right DataFrame (submission_df)\n    how='inner'               # Use 'inner' join (safer) or 'left' if test_df is guaranteed complete\n)\n\n# --- Optional: Clean up columns ---\n# Drop the redundant 'index' column that came from submission_df after merging\n\n# --- Display the Result ---\nprint(\"\\n--- Combined View: Test Text with Predictions ---\")\n# Set display options for better readability\npd.set_option('display.max_colwidth', 150) # Show more of the text\npd.set_option('display.width', 1000)      # Wider display\n\nview_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:31:04.251994Z","iopub.execute_input":"2025-04-15T01:31:04.252780Z","iopub.status.idle":"2025-04-15T01:31:04.263186Z","shell.execute_reply.started":"2025-04-15T01:31:04.252751Z","shell.execute_reply":"2025-04-15T01:31:04.262430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Saving submission file to '{output_csv_path}'...\")\nsubmission_df.to_csv(output_csv_path, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:32:27.036219Z","iopub.execute_input":"2025-04-15T01:32:27.036467Z","iopub.status.idle":"2025-04-15T01:32:27.046452Z","shell.execute_reply.started":"2025-04-15T01:32:27.036449Z","shell.execute_reply":"2025-04-15T01:32:27.045887Z"}},"outputs":[],"execution_count":null}]}